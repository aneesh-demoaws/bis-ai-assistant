<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIS AI Voice Assistant</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body { overflow: hidden; width: 100%; height: 100vh; font-family: system-ui, sans-serif; background: #1a1a2e; color: #fff; }
        #renderCanvas { width: 100%; height: 100%; position: absolute; top: 0; left: 0; z-index: 0; }
        .overlay { position: fixed; z-index: 10; pointer-events: none; }
        .top-bar { top: 20px; left: 0; width: 100%; text-align: center; }
        .top-bar h1 { font-size: 1.5rem; text-shadow: 0 2px 8px rgba(0,0,0,0.5); }
        .top-bar .subtitle { color: #aaa; font-size: 0.9rem; }
        .bottom-panel { bottom: 20px; left: 50%; transform: translateX(-50%); text-align: center; pointer-events: auto; }
        #micBtn { width: 80px; height: 80px; border-radius: 50%; border: none; background: linear-gradient(145deg, #e94560, #c73e54); color: #fff; font-size: 2rem; cursor: pointer; transition: all 0.3s; box-shadow: 0 8px 32px rgba(233,69,96,0.3); }
        #micBtn:hover { transform: scale(1.05); }
        #micBtn.recording { background: linear-gradient(145deg, #4ade80, #22c55e); animation: pulse 1.5s infinite; }
        @keyframes pulse { 0%,100% { box-shadow: 0 0 0 0 rgba(74,222,128,0.4); } 50% { box-shadow: 0 0 0 20px rgba(74,222,128,0); } }
        #status { margin-top: 0.5rem; font-size: 0.95rem; min-height: 1.3rem; text-shadow: 0 1px 4px rgba(0,0,0,0.5); }
        #transcript { position: fixed; bottom: 130px; left: 50%; transform: translateX(-50%); width: 90%; max-width: 500px; max-height: 150px; overflow-y: auto; background: rgba(0,0,0,0.5); backdrop-filter: blur(10px); border-radius: 12px; padding: 0.8rem; z-index: 10; text-align: left; }
        .msg { padding: 0.3rem 0; border-bottom: 1px solid rgba(255,255,255,0.1); font-size: 0.9rem; }
        .msg.user { color: #4ade80; }
        .msg.assistant { color: #60a5fa; }
        .msg.system { color: #fbbf24; font-style: italic; }
        .msg:last-child { border-bottom: none; }
        .preview { opacity: 0.6; }
        #loadScreen { position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: #1a1a2e; display: flex; align-items: center; justify-content: center; flex-direction: column; z-index: 999; }
        .loader { border: 6px solid rgba(96,165,250,0.2); border-top: 6px solid #60a5fa; border-radius: 50%; width: 50px; height: 50px; animation: spin 1s linear infinite; }
        @keyframes spin { 100% { transform: rotate(360deg); } }
        #loadScreen p { margin-top: 1rem; color: #888; }
    </style>
    <script src="https://sdk.amazonaws.com/js/aws-sdk-2.645.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.127.0/build/three.min.js"></script>
    <script src="/assets/3d/host.three.js"></script>
</head>
<body>
    <div id="loadScreen"><div class="loader"></div><p>Loading 3D Avatar...</p></div>
    <div class="overlay top-bar">
        <h1>üéì BIS AI Assistant</h1>
        <p class="subtitle">Bhavans Indian School Voice Chat</p>
    </div>
    <div id="transcript"></div>
    <div class="overlay bottom-panel">
        <button id="micBtn" onclick="toggleRecording()">üé§</button>
        <div id="status">Click to start talking</div>
    </div>

    <script type="module">
        import { OrbitControls } from 'https://cdn.jsdelivr.net/npm/three@0.127.0/examples/jsm/controls/OrbitControls.js';
        import { GLTFLoader } from 'https://cdn.jsdelivr.net/npm/three@0.127.0/examples/jsm/loaders/GLTFLoader.js';

        const ASSET_BASE = '/assets/3d';
        const WS_URL = 'wss://bisai-alb.demoaws.com/voice';
        const renderFn = [];
        let host, scene, camera, clock, renderer;

        // Voice state
        let ws, audioCtx, mediaStream, isRecording = false;
        let playbackCtx, nextPlayTime = 0;
        const pendingBuffers = [];
        let isAssistantSpeaking = false;
        let analyser, analyserData;

        // Make toggleRecording global
        window.toggleRecording = async function() { if (isRecording) stopRecording(); else await startRecording(); };

        await init3D();

        async function init3D() {
            // Scene
            scene = new THREE.Scene();
            clock = new THREE.Clock();
            scene.background = new THREE.Color(0x1a1a2e);
            scene.fog = new THREE.Fog(0x1a1a2e, 0, 12);

            // Renderer
            renderer = new THREE.WebGLRenderer({ antialias: true });
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.outputEncoding = THREE.sRGBEncoding;
            renderer.shadowMap.enabled = true;
            renderer.setClearColor(0x1a1a2e);
            renderer.domElement.id = 'renderCanvas';
            document.body.appendChild(renderer.domElement);

            // Environment map
            new THREE.TextureLoader().setPath(`${ASSET_BASE}/`).load('images/machine_shop.jpg', hdrEquirect => {
                const hdrCube = pmremGen.fromEquirectangular(hdrEquirect);
                hdrEquirect.dispose(); pmremGen.dispose();
                scene.environment = hdrCube.texture;
            });
            const pmremGen = new THREE.PMREMGenerator(renderer);
            pmremGen.compileEquirectangularShader();

            // Camera
            camera = new THREE.PerspectiveCamera(THREE.MathUtils.radToDeg(0.8), window.innerWidth / window.innerHeight, 0.1, 1000);
            const controls = new OrbitControls(camera, renderer.domElement);
            camera.position.set(0, 1.4, 3.1);
            controls.target = new THREE.Vector3(0, 0.8, 0);
            controls.screenSpacePanning = true;
            controls.update();

            window.addEventListener('resize', () => {
                camera.aspect = window.innerWidth / window.innerHeight;
                camera.updateProjectionMatrix();
                renderer.setSize(window.innerWidth, window.innerHeight);
            });

            // Lights
            const hemi = new THREE.HemisphereLight(0xffffff, 0x000000, 0.6);
            hemi.position.set(0, 1, 0);
            scene.add(hemi);
            const dir = new THREE.DirectionalLight(0xffffff);
            dir.position.set(0, 5, 5);
            dir.castShadow = true;
            dir.shadow.mapSize.set(1024, 1024);
            scene.add(dir);
            const dirTarget = new THREE.Object3D();
            dir.add(dirTarget);
            dirTarget.position.set(0, -0.5, -1.0);
            dir.target = dirTarget;

            // Ground
            const ground = new THREE.Mesh(
                new THREE.PlaneBufferGeometry(100, 100),
                new THREE.MeshStandardMaterial({ color: 0x1a1a2e, depthWrite: false, metalness: 0 })
            );
            ground.rotation.x = -Math.PI / 2;
            ground.receiveShadow = true;
            scene.add(ground);

            // Load character
            const gltfLoader = new GLTFLoader();
            const charPath = `${ASSET_BASE}/characters/adult_male/luke/luke.gltf`;
            const animPath = `${ASSET_BASE}/animations/adult_male`;
            const animFiles = ['stand_idle.glb', 'lipsync.glb', 'gesture.glb', 'emote.glb', 'face_idle.glb', 'blink.glb', 'poi.glb'];

            const { character, clips, bindPoseOffset } = await loadCharacter(gltfLoader, charPath, animPath, animFiles);
            character.position.set(0, 0, 0);

            const audioAttachJoint = character.getObjectByName('chardef_c_neckB');
            const lookJoint = character.getObjectByName('charjx_c_look');

            const gestureConfig = await fetch(`${animPath}/gesture.json`).then(r => r.json());
            const poiConfig = await fetch(`${animPath}/poi.json`).then(r => r.json());

            const [idleClips, lipsyncClips, gestureClips, emoteClips, faceClips, blinkClips, poiClips] = clips;

            host = createHost(character, audioAttachJoint, idleClips[0], faceClips[0], lipsyncClips, gestureClips, gestureConfig, emoteClips, blinkClips, poiClips, poiConfig, lookJoint, bindPoseOffset);

            // Render loop
            function render() {
                requestAnimationFrame(render);
                controls.update();
                renderFn.forEach(fn => fn());
                renderer.render(scene, camera);
            }
            render();

            // Hide loading screen
            document.getElementById('loadScreen').style.display = 'none';
        }

        async function loadCharacter(gltfLoader, charFile, animPath, animFiles) {
            function loadAsset(path) {
                return new Promise(resolve => gltfLoader.load(path, resolve));
            }
            const gltf = await loadAsset(charFile);
            const character = gltf.scene;
            scene.add(character);
            const [bindPoseOffset] = gltf.animations;
            if (bindPoseOffset) THREE.AnimationUtils.makeClipAdditive(bindPoseOffset);
            character.traverse(o => { if (o.isMesh) o.castShadow = true; });

            const clips = await Promise.all(animFiles.map(f => loadAsset(`${animPath}/${f}`).then(g => g.animations)));
            return { character, clips, bindPoseOffset };
        }

        function createHost(character, audioAttachJoint, idleClip, faceIdleClip, lipsyncClips, gestureClips, gestureConfig, emoteClips, blinkClips, poiClips, poiConfig, lookJoint, bindPoseOffset) {
            const h = new HOST.HostObject({ owner: character, clock });
            renderFn.push(() => h.update());

            // Animation feature
            h.addFeature(HOST.anim.AnimationFeature);

            // Base idle
            h.AnimationFeature.addLayer('Base');
            h.AnimationFeature.addAnimation('Base', idleClip.name, HOST.anim.AnimationTypes.single, { clip: idleClip });
            h.AnimationFeature.playAnimation('Base', idleClip.name);

            // Face idle
            h.AnimationFeature.addLayer('Face', { blendMode: HOST.anim.LayerBlendModes.Additive });
            THREE.AnimationUtils.makeClipAdditive(faceIdleClip);
            h.AnimationFeature.addAnimation('Face', faceIdleClip.name, HOST.anim.AnimationTypes.single, {
                clip: THREE.AnimationUtils.subclip(faceIdleClip, faceIdleClip.name, 1, faceIdleClip.duration * 30, 30)
            });
            h.AnimationFeature.playAnimation('Face', faceIdleClip.name);

            // Blink
            h.AnimationFeature.addLayer('Blink', { blendMode: HOST.anim.LayerBlendModes.Additive, transitionTime: 0.075 });
            blinkClips.forEach(c => THREE.AnimationUtils.makeClipAdditive(c));
            h.AnimationFeature.addAnimation('Blink', 'blink', HOST.anim.AnimationTypes.randomAnimation, {
                playInterval: 3,
                subStateOptions: blinkClips.map(c => ({ name: c.name, loopCount: 1, clip: c }))
            });
            h.AnimationFeature.playAnimation('Blink', 'blink');

            // Talk layer
            h.AnimationFeature.addLayer('Talk', { transitionTime: 0.75, blendMode: HOST.anim.LayerBlendModes.Additive });
            h.AnimationFeature.setLayerWeight('Talk', 0);
            const talkClip = lipsyncClips.find(c => c.name === 'stand_talk');
            lipsyncClips.splice(lipsyncClips.indexOf(talkClip), 1);
            h.AnimationFeature.addAnimation('Talk', talkClip.name, HOST.anim.AnimationTypes.single, {
                clip: THREE.AnimationUtils.makeClipAdditive(talkClip)
            });
            h.AnimationFeature.playAnimation('Talk', talkClip.name);

            // Viseme layer
            h.AnimationFeature.addLayer('Viseme', { transitionTime: 0.12, blendMode: HOST.anim.LayerBlendModes.Additive });
            h.AnimationFeature.setLayerWeight('Viseme', 0);
            const blendStateOptions = lipsyncClips.map(c => {
                THREE.AnimationUtils.makeClipAdditive(c);
                return { name: c.name, clip: THREE.AnimationUtils.subclip(c, c.name, 1, 2, 30), weight: 0 };
            });
            h.AnimationFeature.addAnimation('Viseme', 'visemes', HOST.anim.AnimationTypes.freeBlend, { blendStateOptions });
            h.AnimationFeature.playAnimation('Viseme', 'visemes');

            // Gesture layer
            h.AnimationFeature.addLayer('Gesture', { transitionTime: 0.5, blendMode: HOST.anim.LayerBlendModes.Additive });
            gestureClips.forEach(clip => {
                const config = gestureConfig[clip.name];
                THREE.AnimationUtils.makeClipAdditive(clip);
                if (config) {
                    config.queueOptions.forEach(opt => {
                        opt.clip = THREE.AnimationUtils.subclip(clip, `${clip.name}_${opt.name}`, opt.from, opt.to, 30);
                    });
                    h.AnimationFeature.addAnimation('Gesture', clip.name, HOST.anim.AnimationTypes.queue, config);
                } else {
                    h.AnimationFeature.addAnimation('Gesture', clip.name, HOST.anim.AnimationTypes.single, { clip });
                }
            });

            // POI
            poiConfig.forEach(config => {
                h.AnimationFeature.addLayer(config.name, { blendMode: HOST.anim.LayerBlendModes.Additive });
                config.blendStateOptions.forEach(cc => {
                    const clip = poiClips.find(c => c.name === cc.clip);
                    THREE.AnimationUtils.makeClipAdditive(clip);
                    cc.clip = THREE.AnimationUtils.subclip(clip, clip.name, 1, 2, 30);
                });
                h.AnimationFeature.addAnimation(config.name, config.animation, HOST.anim.AnimationTypes.blend2d, { ...config });
                h.AnimationFeature.playAnimation(config.name, config.animation);
                config.reference = character.getObjectByName(config.reference.replace(':', ''));
            });

            // BindPoseOffset
            if (bindPoseOffset) {
                h.AnimationFeature.addLayer('BindPoseOffset', { blendMode: HOST.anim.LayerBlendModes.Additive });
                h.AnimationFeature.addAnimation('BindPoseOffset', bindPoseOffset.name, HOST.anim.AnimationTypes.single, {
                    clip: THREE.AnimationUtils.subclip(bindPoseOffset, bindPoseOffset.name, 1, 2, 30)
                });
                h.AnimationFeature.playAnimation('BindPoseOffset', bindPoseOffset.name);
            }

            // POI feature - look at camera
            h.addFeature(HOST.PointOfInterestFeature, false,
                { target: camera, lookTracker: lookJoint, scene },
                { layers: poiConfig },
                { layers: [{ name: 'Blink' }] }
            );

            return h;
        }

        // --- Audio-driven viseme animation ---
        const visemeMap = ['sil', 'a', 'o', 'e', 'i', 'u', 'f', 'k', 's', 'T', 't', 'S', 'r', 'p', 'E', 'O', '@'];
        let visemeFrame;

        function startVisemeAnim() {
            if (!host || !analyser) return;
            host.AnimationFeature.setLayerWeight('Viseme', 1);
            host.AnimationFeature.setLayerWeight('Talk', 1);
            animateVisemes();
        }

        function stopVisemeAnim() {
            if (visemeFrame) { cancelAnimationFrame(visemeFrame); visemeFrame = null; }
            if (host) {
                // Reset all viseme weights to 0
                try { visemeMap.forEach(v => host.AnimationFeature.setAnimationBlendWeight('Viseme', 'visemes', v, 0, 0.1)); } catch(e) {}
                host.AnimationFeature.setLayerWeight('Viseme', 0);
                host.AnimationFeature.setLayerWeight('Talk', 0);
            }
        }

        function animateVisemes() {
            if (!analyser || !isAssistantSpeaking) { stopVisemeAnim(); return; }
            analyser.getByteFrequencyData(analyserData);

            // Overall energy
            let total = 0;
            for (let i = 1; i < 16; i++) total += analyserData[i];
            const energy = total / (15 * 255);

            // Frequency band mapping
            const low = Math.min((analyserData[1] + analyserData[2] + analyserData[3]) / (3 * 180), 1);
            const mid = Math.min((analyserData[4] + analyserData[5] + analyserData[6]) / (3 * 200), 1);
            const high = Math.min((analyserData[8] + analyserData[10] + analyserData[12]) / (3 * 220), 1);

            const weights = {};
            visemeMap.forEach(v => weights[v] = 0);

            if (energy > 0.05) {
                weights['a'] = low * 0.8;
                weights['o'] = low * 0.4;
                weights['O'] = low * 0.3;
                weights['e'] = mid * 0.6;
                weights['E'] = mid * 0.4;
                weights['i'] = mid * 0.3;
                weights['u'] = (low + mid) * 0.2;
                weights['s'] = high * 0.5;
                weights['f'] = high * 0.4;
                weights['T'] = high * 0.3;
                weights['k'] = mid * 0.2;
                weights['p'] = energy > 0.15 ? 0.3 : 0;
                weights['r'] = mid * 0.2;
                weights['@'] = energy * 0.3;
            } else {
                weights['sil'] = 1;
            }

            try {
                visemeMap.forEach(v => {
                    host.AnimationFeature.setAnimationBlendWeight('Viseme', 'visemes', v, weights[v] || 0, 0.05);
                });
            } catch(e) { console.warn('viseme error', e); }

            visemeFrame = requestAnimationFrame(animateVisemes);
        }

        // --- Voice WebSocket ---
        function setupAnalyser() {
            if (!playbackCtx || analyser) return;
            analyser = playbackCtx.createAnalyser();
            analyser.fftSize = 128;
            analyserData = new Uint8Array(analyser.frequencyBinCount);
        }

        async function startRecording() {
            try {
                setStatus('Connecting...');
                ws = new WebSocket(WS_URL);
                ws.onopen = async () => {
                    setStatus('üéôÔ∏è Listening... speak anytime');
                    audioCtx = new AudioContext({ sampleRate: 16000 });
                    playbackCtx = new AudioContext({ sampleRate: 16000 });
                    nextPlayTime = playbackCtx.currentTime;
                    analyser = null;
                    mediaStream = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1, echoCancellation: true, noiseSuppression: true, autoGainControl: true } });
                    const source = audioCtx.createMediaStreamSource(mediaStream);
                    const processor = audioCtx.createScriptProcessor(2048, 1, 1);
                    processor.onaudioprocess = (e) => {
                        if (ws?.readyState === WebSocket.OPEN) {
                            const pcm = e.inputBuffer.getChannelData(0);
                            const int16 = new Int16Array(pcm.length);
                            for (let i = 0; i < pcm.length; i++) int16[i] = Math.max(-32768, Math.min(32767, pcm[i] * 32768));
                            ws.send(JSON.stringify({ type: 'audio', data: arrayBufferToBase64(int16.buffer) }));
                        }
                    };
                    source.connect(processor);
                    processor.connect(audioCtx.destination);
                    isRecording = true;
                    updateMicButton();
                };
                ws.onmessage = (e) => {
                    const msg = JSON.parse(e.data);
                    if (msg.type === 'audio') {
                        scheduleAudio(msg.data);
                        if (!isAssistantSpeaking) { isAssistantSpeaking = true; updateMicButton(); startVisemeAnim(); setStatus('üîä Speaking... (interrupt anytime)'); }
                    } else if (msg.type === 'transcript') { updateTranscript(msg.role, msg.text, msg.is_final);
                    } else if (msg.type === 'interruption') { clearAudioQueue(); isAssistantSpeaking = false; updateMicButton(); stopVisemeAnim(); setStatus('üéôÔ∏è Listening...'); addSystemMessage('‚ö° Interrupted');
                    } else if (msg.type === 'response_end') { isAssistantSpeaking = false; updateMicButton(); stopVisemeAnim(); setStatus('üéôÔ∏è Listening... speak anytime');
                    } else if (msg.type === 'error') { setStatus('‚ùå ' + msg.message); }
                };
                ws.onclose = () => { stopRecording(); setStatus('Disconnected - click to reconnect'); };
                ws.onerror = () => { stopRecording(); setStatus('Connection error'); };
            } catch (err) { setStatus('Error: ' + err.message); }
        }

        function stopRecording() {
            if (ws?.readyState === WebSocket.OPEN) { ws.send(JSON.stringify({ type: 'stop' })); ws.close(); }
            if (mediaStream) mediaStream.getTracks().forEach(t => t.stop());
            if (audioCtx) audioCtx.close().catch(() => {});
            if (playbackCtx) playbackCtx.close().catch(() => {});
            clearAudioQueue(); stopVisemeAnim();
            isRecording = false; isAssistantSpeaking = false; analyser = null;
            updateMicButton(); setStatus('Click to start talking');
        }

        function arrayBufferToBase64(buf) { const b = new Uint8Array(buf); let s = ''; for (let i = 0; i < b.length; i++) s += String.fromCharCode(b[i]); return btoa(s); }
        function base64ToInt16Array(b64) { const s = atob(b64); const b = new Uint8Array(s.length); for (let i = 0; i < s.length; i++) b[i] = s.charCodeAt(i); return new Int16Array(b.buffer); }

        function scheduleAudio(b64) {
            if (!playbackCtx) return;
            setupAnalyser();
            const int16 = base64ToInt16Array(b64);
            const f32 = new Float32Array(int16.length);
            for (let i = 0; i < int16.length; i++) f32[i] = int16[i] / 32768;
            const buffer = playbackCtx.createBuffer(1, f32.length, 16000);
            buffer.copyToChannel(f32, 0);
            const src = playbackCtx.createBufferSource();
            src.buffer = buffer;
            src.connect(analyser);
            analyser.connect(playbackCtx.destination);
            const now = playbackCtx.currentTime;
            if (nextPlayTime < now) nextPlayTime = now;
            src.start(nextPlayTime);
            pendingBuffers.push(src);
            nextPlayTime += buffer.duration;
            src.onended = () => { const i = pendingBuffers.indexOf(src); if (i > -1) pendingBuffers.splice(i, 1); };
        }

        function clearAudioQueue() { pendingBuffers.forEach(s => { try { s.stop(); } catch(e){} }); pendingBuffers.length = 0; if (playbackCtx) nextPlayTime = playbackCtx.currentTime; }

        let currentPreview = null;
        function updateTranscript(role, text, isFinal) {
            const div = document.getElementById('transcript');
            if (!isFinal) {
                if (!currentPreview || currentPreview.dataset.role !== role) { currentPreview = document.createElement('div'); currentPreview.className = 'msg ' + role + ' preview'; currentPreview.dataset.role = role; div.appendChild(currentPreview); }
                currentPreview.textContent = (role === 'user' ? 'üë§ ' : 'ü§ñ ') + text + '...';
            } else {
                if (currentPreview && currentPreview.dataset.role === role) { currentPreview.classList.remove('preview'); currentPreview.textContent = (role === 'user' ? 'üë§ ' : 'ü§ñ ') + text; currentPreview = null; }
                else { const p = document.createElement('div'); p.className = 'msg ' + role; p.textContent = (role === 'user' ? 'üë§ ' : 'ü§ñ ') + text; div.appendChild(p); }
            }
            div.scrollTop = div.scrollHeight;
        }
        function addSystemMessage(text) { const div = document.getElementById('transcript'); const p = document.createElement('div'); p.className = 'msg system'; p.textContent = text; div.appendChild(p); div.scrollTop = div.scrollHeight; }
        function updateMicButton() { const btn = document.getElementById('micBtn'); btn.classList.remove('recording'); if (!isRecording) btn.textContent = 'üé§'; else { btn.classList.add('recording'); btn.textContent = 'üéôÔ∏è'; } }
        function setStatus(s) { document.getElementById('status').textContent = s; }
    </script>
</body>
</html>
